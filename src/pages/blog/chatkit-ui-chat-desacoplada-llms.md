---
layout: ../../layouts/PostLayout.astro
title: "Dise√±ar una UI de chat que no sabe nada de LLMs (y por qu√© eso importa)"
date: "2025-12-27"
description: "Separar la UI de chat de la l√≥gica de los LLMs no es una optimizaci√≥n: es una decisi√≥n arquitect√≥nica que evita acoplamiento, suposiciones impl√≠citas y deuda t√©cnica."
image: /images/posts/chatkit-ui.png
imageAlt: Ilustraci√≥n conceptual de una interfaz de chat desacoplada de la l√≥gica de IA
---


# Dise√±ar una UI de chat que no sabe nada de LLMs (y por qu√© eso importa)

Durante los √∫ltimos d√≠as he estado construyendo **ChatKit**, un peque√±o SDK en SwiftUI para renderizar interfaces de chat.  
No para ‚Äúhablar con ChatGPT‚Äù.  
No para ‚Äúintegrar IA‚Äù.  
Simplemente para **mostrar mensajes**.

Y esa distinci√≥n, aunque parezca menor, cambia completamente el dise√±o.

---

## El problema habitual con las UIs de chat

La mayor√≠a de interfaces de chat actuales ‚Äîespecialmente las que rodean a LLMs‚Äî cometen el mismo error de base:

> **La UI intenta entender la conversaci√≥n.**

Decide cu√°ndo toca responder el asistente.  
Asume alternancia usuario ‚Üí assistant.  
Gestiona estados de red, tokens, streaming, retries.  
Se acopla al proveedor (OpenAI, Anthropic, local, etc.).

El resultado es una UI:
- dif√≠cil de reutilizar  
- dif√≠cil de testear  
- dif√≠cil de evolucionar  
- innecesariamente acoplada al backend  

Yo quer√≠a justo lo contrario.

---

## Principio fundamental: la UI no piensa

ChatKit parte de una idea muy simple:

> **La UI no decide nada. Solo renderiza lo que le pasan.**

Si recibe:
- dos mensajes del assistant seguidos ‚Üí renderiza dos  
- un mensaje system en mitad ‚Üí lo muestra  
- cero mensajes ‚Üí no inventa nada  

No hay reglas impl√≠citas.  
No hay ‚Äúl√≥gica conversacional‚Äù.  
No hay suposiciones.

La conversaci√≥n **no existe** como concepto.  
Solo existe una lista ordenada de mensajes.

---

## Roles como datos, no como reglas

En ChatKit, un mensaje tiene un rol:

- `user`
- `assistant`
- `system`

El rol **solo afecta a c√≥mo se renderiza** (alineaci√≥n, estilo, color).  
No define qui√©n habla despu√©s ni cu√°ntas veces puede hacerlo.

Si el consumidor inyecta:
- dos respuestas consecutivas del assistant  
- un system message en mitad  
- o ning√∫n assistant en absoluto  

ChatKit no lo cuestiona. Lo renderiza.

---

## El ViewModel como frontera expl√≠cita

La interacci√≥n con el exterior ocurre a trav√©s de un `ChatViewModel` inicializado as√≠:

- mensajes iniciales
- quick prompts opcionales
- un closure `onSend`

Cuando el usuario env√≠a un mensaje, ChatKit:
- lo a√±ade al array
- notifica mediante el closure

Qu√© ocurre despu√©s **no es responsabilidad del SDK**:
- llamadas a un LLM  
- retries  
- streaming  
- errores  
- m√∫ltiples respuestas  

ChatKit no env√≠a requests.  
Notifica eventos.

---

## Estados m√≠nimos y visuales

ChatKit define estados **solo para la UI**:

- ready  
- awaitingAssistant  
- error  

No hay estados m√°gicos.  
No hay inferencias impl√≠citas.

Si el consumidor quiere:
- bloquear input  
- mostrar animaciones  
- permitir m√∫ltiples respuestas  

lo decide fuera.

---

## Sobre el ‚Äústreaming‚Äù (honestidad t√©cnica)

Aunque el modelo define estados como `streaming`, a d√≠a de hoy **ChatKit no implementa un pipeline oficial de tokens**.

Esto es intencional.

Antes de a√±adir complejidad, quise validar que el dise√±o:
- no se romp√≠a en el caso m√°s simple  
- no impon√≠a decisiones irreversibles  

El dise√±o est√° preparado para crecer hacia streaming, pero **no lo promete todav√≠a**.

---

## Apariencia: configuraci√≥n, no temas cerrados

ChatKit expone la apariencia como configuraci√≥n:

- colores  
- tipograf√≠a  
- padding  
- estilos de burbuja  

No impone temas cerrados ni branding propio.

La app contenedora decide:
- colores light  
- colores dark  
- o usar un √∫nico esquema  

El SDK no intenta ser m√°s listo que el consumidor.

---

## Animaciones: sutiles y opcionales

Las animaciones est√°n pensadas como:
- transiciones suaves  
- feedback visual m√≠nimo  

No afectan al modelo de datos ni al flujo.

Si ma√±ana se eliminan, el sistema sigue funcionando.

---

## C√≥mo probarlo contra un LLM real (sin buenas pr√°cticas)

Para validar que el dise√±o funciona fuera del entorno te√≥rico, conect√© ChatKit directamente a la API de OpenAI desde **un proyecto nuevo**, import√°ndolo como Swift Package.

El objetivo **no es ense√±ar a integrar OpenAI correctamente**, sino demostrar que el SDK funciona con un backend real.

Para ello cre√© un `OpenAIClient` m√≠nimo

```swift
import Foundation
import ChatKit

struct OpenAIClient {

    let apiKey: String

    func send(
        messages: [ChatMessage],
        completion: @escaping (Result<String, Error>) -> Void
    ) {
        let url = URL(string: "https://api.openai.com/v1/chat/completions")!

        let payload: [String: Any] = [
            "model": "gpt-4o-mini",
            "messages": messages.map {
                [
                    "role": mapRole($0.role),
                    "content": $0.content
                ]
            }
        ]

        var request = URLRequest(url: url)
        request.httpMethod = "POST"
        request.setValue("Bearer \(apiKey)", forHTTPHeaderField: "Authorization")
        request.setValue("application/json", forHTTPHeaderField: "Content-Type")
        request.httpBody = try? JSONSerialization.data(withJSONObject: payload)

        URLSession.shared.dataTask(with: request) { data, _, error in
            if let error {
                completion(.failure(error))
                return
            }

            guard
                let data,
                let json = try? JSONSerialization.jsonObject(with: data) as? [String: Any],
                let choices = json["choices"] as? [[String: Any]],
                let message = choices.first?["message"] as? [String: Any],
                let content = message["content"] as? String
            else {
                completion(.failure(NSError(domain: "OpenAI", code: -1)))
                return
            }

            completion(.success(content))
        }.resume()
    }

    private func mapRole(_ role: ChatRole) -> String {
        switch role {
        case .user:
            return "user"
        case .assistant:
            return "assistant"
        case .system:
            return "system"
        }
    }
}

```

y hardcode√© la API key directamente en el `ContentView`



```swift
import SwiftUI
import ChatKit

struct ContentView: View {

    @StateObject private var viewModel: ChatViewModel
    private let brandAppearance: ChatAppearancePair

    init() {
        var vmRef: ChatViewModel?

        let vm = ChatViewModel(
            initialMessages: [
                ChatMessage(
                    role: .assistant,
                    content: "Hi! Ask me anything.",
                    status: .completed
                )
            ],
            quickPrompts: [
                QuickPrompt(id: UUID(), title: "Resume this", message: "Resume this"),
                QuickPrompt(id: UUID(), title: "Give examples", message: "Give examples"),
                QuickPrompt(id: UUID(), title: "Explain like I'm 5", message: "Explain like I'm 5")
            ],
            onSend: { message in
                DispatchQueue.main.asyncAfter(deadline: .now() + 1.0) {
                    let response = ChatMessage(
                        role: .assistant,
                        content: "Fake LLM response to: \(message.content)",
                        status: .completed
                    )
                    vmRef?.appendMessage(response)
                }
            }
        )
        
        vmRef = vm

        let lightAppearance = ChatAppearance(
            background: Color(.systemGroupedBackground),
            font: .callout,
            contentPadding: 14,
            userBubbleBackground: Color(red: 255/255, green: 204/255, blue: 0/255).opacity(0.25), // Vueling yellow
            assistantBubbleBackground: Color.gray.opacity(0.12),
            userForeground: .black,
            assistantForeground: .primary,
            cornerRadius: 18
        )

        let darkAppearance = ChatAppearance(
            background: Color(.black),
            font: .callout,
            contentPadding: 14,
            userBubbleBackground: Color(red: 255/255, green: 204/255, blue: 0/255).opacity(0.35),
            assistantBubbleBackground: Color.white.opacity(0.12),
            userForeground: .black,
            assistantForeground: .white,
            cornerRadius: 18
        )

        brandAppearance = ChatAppearancePair(
            light: lightAppearance,
            dark: darkAppearance
        )

        _viewModel = StateObject(wrappedValue: vm)
    }

    var body: some View {
        ChatView(
            viewModel: viewModel,
            appearance: brandAppearance,
            layout: .compact,
            behavior: .default
        )
    }
}
```

El `ChatViewModel` notifica cuando el usuario env√≠a un mensaje.  
El cliente llama a OpenAI y, cuando llega la respuesta, la inyecta como un mensaje con rol `.assistant`.

ChatKit no sabe:
- que OpenAI existe  
- que hay una API key  
- que hay una request en curso  

Solo renderiza lo que recibe.

Aqu√≠ el objetivo es otro: demostrar que el dise√±o funciona cuando deja de ser te√≥rico.

Y funciona.

---

## Conclusi√≥n

ChatKit no intenta ser un ‚Äúchat inteligente‚Äù.  
Intenta ser una **UI predecible** en un ecosistema lleno de suposiciones impl√≠citas.

Separar la UI del backend no es una optimizaci√≥n prematura.  
Es una decisi√≥n estructural.

Y, en proyectos reales, suele marcar la diferencia.

---

üöß **Status**

Este proyecto est√° en desarrollo activo.  
La API p√∫blica no se considera estable antes de la versi√≥n 1.0.

- [Repositorio Github Package ChatKit ](https://medium.com/@eu.barquin/how-to-run-ai-models-locally-on-ios-with-core-ml-part-1-f69bd82de69c)
